{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466204de",
   "metadata": {},
   "source": [
    "# The Lazy Book Report\n",
    "\n",
    "Your professor has assigned a book report on \"The Red-Headed League\" by Arthur Conan Doyle. \n",
    "\n",
    "You haven't read the book. And out of stubbornness, you won't.\n",
    "\n",
    "But you *have* learned NLP. Let's use it to answer the professor's questions without reading.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's fetch the text from Project Gutenberg and prepare it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a46884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story loaded: 4000 words in 3 sections\n",
      "Section sizes: [1333, 1333, 1334]\n"
     ]
    }
   ],
   "source": [
    "# Fetch and prepare text - RUN THIS CELL FIRST\n",
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "url = 'https://www.gutenberg.org/files/1661/1661-0.txt'\n",
    "req = urllib.request.Request(url, headers={'User-Agent': 'Python-urllib'})\n",
    "with urllib.request.urlopen(req, timeout=30) as resp:\n",
    "    text = resp.read().decode('utf-8')\n",
    "\n",
    "# Strip Gutenberg boilerplate\n",
    "text = text.split('*** START OF')[1].split('***')[1]\n",
    "text = text.split('*** END OF')[0]\n",
    "\n",
    "# Extract \"The Red-Headed League\" story (it's the second story in the collection)\n",
    "matches = list(re.finditer(r'THE RED-HEADED LEAGUE', text, re.IGNORECASE))\n",
    "story_start = matches[1].end()\n",
    "story_text = text[story_start:]\n",
    "story_end = re.search(r'\\n\\s*III\\.\\s*\\n', story_text)\n",
    "story_text = story_text[:story_end.start()] if story_end else story_text\n",
    "\n",
    "# Split into 3 sections by word count\n",
    "words = story_text.split()[:4000]\n",
    "section_size = len(words) // 3\n",
    "sections = [\n",
    "    ' '.join(words[:section_size]),\n",
    "    ' '.join(words[section_size:2*section_size]),\n",
    "    ' '.join(words[2*section_size:])\n",
    "]\n",
    "\n",
    "print(f\"Story loaded: {len(words)} words in {len(sections)} sections\")\n",
    "print(f\"Section sizes: {[len(s.split()) for s in sections]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e757a",
   "metadata": {},
   "source": [
    "## Professor's Questions\n",
    "\n",
    "Your professor wants you to answer 5 questions about the story. Let's use NLP to find the answers.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 1: Writing Style\n",
    "\n",
    "> \"This text is from the 1890s. What makes it different from modern writing?\"\n",
    "\n",
    "**NLP Method:** Use preprocessing to compute text statistics. Tokenize the text and calculate:\n",
    "- Vocabulary richness (unique words / total words)\n",
    "- Average sentence length\n",
    "- Average word length\n",
    "\n",
    "**Hint:** Formal, literary writing typically shows higher vocabulary richness and longer sentences than modern casual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1710e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: compute text statistics\n",
    "# You'll need: import string, import re\n",
    "# - Tokenize: remove punctuation, lowercase\n",
    "# - Sentences: split on sentence-ending punctuation\n",
    "# Calculate vocab_richness, avg_sentence_length, avg_word_length\n",
    "import string \n",
    "import re \n",
    "\n",
    "def compute_text_statistics(text):\n",
    "    # Tokenize\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [word.lower().translate(translator) for word in text.split() if word.translate(translator)]\n",
    "    \n",
    "    # Unique words\n",
    "    unique_words = set(tokens)\n",
    "    vocab_richness = len(unique_words) / len(tokens) if tokens else 0\n",
    "    \n",
    "    # Sentences\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s for s in sentences if s.strip()]\n",
    "    avg_sentence_length = len(tokens) / len(sentences) if sentences else 0\n",
    "    \n",
    "    # Average word length\n",
    "    total_word_length = sum(len(word) for word in tokens)\n",
    "    avg_word_length = total_word_length / len(tokens) if tokens else 0\n",
    "    \n",
    "    return {\n",
    "        'vocab_richness': vocab_richness,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_word_length': avg_word_length\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5545405",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2: Main Characters\n",
    "\n",
    "> \"Who are the main characters in this story?\"\n",
    "\n",
    "**NLP Method:** Use Named Entity Recognition (NER) to extract PERSON entities.\n",
    "\n",
    "**Hint:** Use spaCy's `en_core_web_sm` model. Process the text and filter entities where `ent.label_ == 'PERSON'`. Count how often each name appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42832c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "332a59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: extract PERSON entities using spaCy NER\n",
    "# You'll need: import spacy, nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/characters.txt\", \"w\") as f:\n",
    "#     for name in your_character_list:\n",
    "#         f.write(f\"{name}\\n\")\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_person_entities(text):\n",
    "    doc = nlp(text)\n",
    "    persons = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            persons.add(ent.text)\n",
    "    return sorted(persons)  \n",
    "\n",
    "# Process each section\n",
    "for i, section in enumerate(sections):\n",
    "    stats = compute_text_statistics(section)\n",
    "    characters = extract_person_entities(section)\n",
    "    \n",
    "    # Save statistics\n",
    "    with open(f\"output/section_{i+1}_stats.txt\", \"w\") as f:\n",
    "        for key, value in stats.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    # Save character names\n",
    "    with open(f\"output/section_{i+1}_characters.txt\", \"w\") as f:\n",
    "        for name in characters:\n",
    "            f.write(f\"{name}\\n\")\n",
    "\n",
    "# Characters over all sections\n",
    "all_characters = set()\n",
    "for section in sections:\n",
    "    all_characters.update(extract_person_entities(section))\n",
    "with open(\"output/characters.txt\", \"w\") as f:\n",
    "    for name in sorted(all_characters):\n",
    "        f.write(f\"{name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732e661",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3: Story Locations\n",
    "\n",
    "> \"Where does the story take place?\"\n",
    "\n",
    "**NLP Method:** Use Named Entity Recognition (NER) to extract location entities (GPE and LOC).\n",
    "\n",
    "**Hint:** Filter entities where `ent.label_` is 'GPE' (geopolitical entity) or 'LOC' (location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3f8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: extract GPE and LOC entities using spaCy NER\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/locations.txt\", \"w\") as f:\n",
    "#     for place in your_locations_list:\n",
    "#         f.write(f\"{place}\\n\")\n",
    "\n",
    "def extract_location_entities(text):\n",
    "    doc = nlp(text)\n",
    "    locations = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"GPE\", \"LOC\"}:\n",
    "            locations.add(ent.text)\n",
    "    return sorted(locations)\n",
    "\n",
    "# Process each section for locations\n",
    "for i, section in enumerate(sections):\n",
    "    locations = extract_location_entities(section)\n",
    "    \n",
    "    # Save location names\n",
    "    with open(f\"output/section_{i+1}_locations.txt\", \"w\") as f:\n",
    "        for place in locations:\n",
    "            f.write(f\"{place}\\n\")\n",
    "\n",
    "# Locations over all sections\n",
    "all_locations = set()\n",
    "for section in sections:\n",
    "    all_locations.update(extract_location_entities(section))\n",
    "with open(\"output/locations.txt\", \"w\") as f:\n",
    "    for place in sorted(all_locations):\n",
    "        f.write(f\"{place}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b228d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4: Wilson's Business\n",
    "\n",
    "> \"What is Wilson's business?\"\n",
    "\n",
    "**NLP Method:** Use TF-IDF similarity to find which section discusses Wilson's business.\n",
    "\n",
    "**Hint:** Create a TF-IDF vectorizer, fit it on the 3 sections, then transform your query using the same vectorizer (`.transform()`, not `.fit_transform()` - you want to use the vocabulary learned from the sections). Find which section has the highest cosine similarity and read it to find the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24f7fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: use TF-IDF similarity to find the relevant section\n",
    "# You'll need: from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#              from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/business.txt\", \"w\") as f:\n",
    "#     f.write(\"Wilson's business is: ...\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_relevant_section(sections, query):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sections + [query])\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    most_relevant_index = cosine_similarities.argmax()\n",
    "    return most_relevant_index\n",
    "\n",
    "query = \"Wilson's business\"\n",
    "relevant_index = find_relevant_section(sections, query)\n",
    "business_section = sections[relevant_index]\n",
    "with open(\"output/business.txt\", \"w\") as f:\n",
    "    f.write(f\"Wilson's business is described in section {relevant_index + 1}:\\n\\n\")\n",
    "    f.write(business_section)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85452bf1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 5: Wilson's Work Routine\n",
    "\n",
    "> \"What is Wilson's daily work routine for the League?\"\n",
    "\n",
    "**NLP Method:** Use TF-IDF similarity to find which section discusses Wilson's work routine.\n",
    "\n",
    "**Hint:** Similar to Question 4 - use TF-IDF to find the section that best matches your query about work routine. The answer includes what Wilson had to do and what eventually happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ddea14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: use TF-IDF similarity to find the relevant section\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/routine.txt\", \"w\") as f:\n",
    "#     f.write(\"Wilson's work routine: ...\\n\")\n",
    "#     f.write(\"What happened: ...\\n\")\n",
    "query = \"Wilson's work routine and what happened\"\n",
    "relevant_index = find_relevant_section(sections, query)\n",
    "routine_section = sections[relevant_index]\n",
    "with open(\"output/routine.txt\", \"w\") as f:\n",
    "    f.write(f\"Wilson's work routine and what happened is described in section {relevant_index + 1}:\\n\\n\")\n",
    "    f.write(routine_section)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
